{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备工作\n",
    "### 环境准备\n",
    "\n",
    "请确保完成以下依赖包的安装，并且通过下面代码来导入与验证。运行成功后，你会看到一个新的窗口，其展示了一张空白的figure。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, List\n",
    "\n",
    "# display the plot in a separate window\n",
    "%matplotlib tk\n",
    "\n",
    "np.random.seed(12)\n",
    "\n",
    "# create a figure and axis\n",
    "plt.ion()\n",
    "fig = plt.figure(figsize=(12, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据集准备\n",
    "\n",
    "你将使用以下二维数据集来训练逻辑分类器，并观察随着训练的进行，线性分割面的变化。\n",
    "\n",
    "该数据集包含两个特征和一个标签，其中标签 $ y \\in \\{-1,1\\} $。\n",
    "\n",
    "请执行下面的代码以加载数据集并对其进行可视化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_generator import gen_2D_dataset\n",
    "\n",
    "x_train, y_train = gen_2D_dataset(100, 100, noise = 0)\n",
    "x_test, y_test = gen_2D_dataset(50, 50, noise = 0.7) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vis_util import visualize_2D_dataset, visualize_2D_border\n",
    "\n",
    "visualize_2D_dataset(x_train, y_train)\n",
    "visualize_2D_dataset(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![figure1](img/Figure_1.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 逻辑回归 (10 pts)\n",
    "\n",
    "在这一部分，你将学习并完成逻辑回归相关代码的编写与训练。\n",
    "\n",
    "在运行这部分代码之前，请确保你已经完成了 `logistics.py` 文件的代码补全。\n",
    "\n",
    "完成后，运行以下代码，你会看到一张figure来展示$||w||$，loss和决策边界的变化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0, loss: 0.6807266354549091, w_module: 1.86503565781866\n",
      "iter: 10, loss: 0.6047988783795631, w_module: 1.8481489888845297\n",
      "iter: 20, loss: 0.5660832091387538, w_module: 1.8416649857462604\n",
      "iter: 30, loss: 0.5309314279730247, w_module: 1.852628762958914\n",
      "iter: 40, loss: 0.49873990315881916, w_module: 1.8798975391623567\n",
      "iter: 50, loss: 0.4692664495478231, w_module: 1.9209134420153\n",
      "iter: 60, loss: 0.4422816596337917, w_module: 1.9730364389416215\n",
      "iter: 70, loss: 0.4175668558824793, w_module: 2.033845587213826\n",
      "iter: 80, loss: 0.39491642826729456, w_module: 2.101235926921896\n",
      "iter: 90, loss: 0.3741392267477511, w_module: 2.173440566023107\n",
      "iter: 100, loss: 0.3550591551622385, w_module: 2.249013982747482\n",
      "iter: 110, loss: 0.33751516416978994, w_module: 2.326796185724719\n",
      "iter: 120, loss: 0.32136081353797313, w_module: 2.4058701532487836\n",
      "iter: 130, loss: 0.30646354486334976, w_module: 2.4855197670708282\n",
      "iter: 140, loss: 0.29270377665622754, w_module: 2.5651918359489785\n",
      "iter: 150, loss: 0.2799739069601067, w_module: 2.6444635594027073\n",
      "iter: 160, loss: 0.26817728569910904, w_module: 2.723015541903783\n",
      "iter: 170, loss: 0.2572272001987447, w_module: 2.80060987039218\n",
      "iter: 180, loss: 0.24704590264325801, w_module: 2.877072542545002\n",
      "iter: 190, loss: 0.23756369713488554, w_module: 2.952279503475223\n",
      "iter: 200, loss: 0.2287180959040338, w_module: 3.026145610160481\n",
      "iter: 210, loss: 0.22045304847935793, w_module: 3.098615938595309\n",
      "iter: 220, loss: 0.21271824371709563, w_module: 3.169658949317714\n",
      "iter: 230, loss: 0.20546848204891383, w_module: 3.239261119331016\n",
      "iter: 240, loss: 0.19866311376522533, w_module: 3.3074227276783668\n",
      "iter: 250, loss: 0.19226553831674806, w_module: 3.3741545473463113\n",
      "iter: 260, loss: 0.1862427592713795, w_module: 3.439475248947205\n",
      "iter: 270, loss: 0.1805649895436243, w_module: 3.503409363592368\n",
      "iter: 280, loss: 0.17520530170196288, w_module: 3.565985685435037\n",
      "iter: 290, loss: 0.17013931847203295, w_module: 3.627236020279333\n",
      "iter: 300, loss: 0.16534493893221425, w_module: 3.6871942069037673\n",
      "iter: 310, loss: 0.16080209630385894, w_module: 3.745895353552433\n",
      "iter: 320, loss: 0.15649254364519968, w_module: 3.8033752443787776\n",
      "iter: 330, loss: 0.1523996641497105, w_module: 3.8596698802551885\n",
      "iter: 340, loss: 0.1485083031168994, w_module: 3.9148151258884014\n",
      "iter: 350, loss: 0.14480461900122626, w_module: 3.968846441074419\n",
      "iter: 360, loss: 0.14127595125119785, w_module: 4.0217986785501445\n",
      "iter: 370, loss: 0.13791070292575736, w_module: 4.073705934533657\n",
      "iter: 380, loss: 0.13469823632016772, w_module: 4.12460144090855\n",
      "iter: 390, loss: 0.13162878005067505, w_module: 4.17451749026865\n",
      "iter: 400, loss: 0.1286933462386804, w_module: 4.223485386828482\n",
      "iter: 410, loss: 0.12588365660340467, w_module: 4.2715354176234905\n",
      "iter: 420, loss: 0.12319207641954594, w_module: 4.318696839551391\n",
      "iter: 430, loss: 0.12061155542551867, w_module: 4.36499787870358\n",
      "iter: 440, loss: 0.11813557488069022, w_module: 4.410465739151573\n",
      "iter: 450, loss: 0.11575810006855622, w_module: 4.455126618925515\n",
      "iter: 460, loss: 0.11347353762879621, w_module: 4.499005731379726\n",
      "iter: 470, loss: 0.11127669717620334, w_module: 4.542127330507203\n",
      "iter: 480, loss: 0.10916275672998427, w_module: 4.584514739059417\n",
      "iter: 490, loss: 0.10712723153411047, w_module: 4.6261903785643\n",
      "iter: 500, loss: 0.10516594589934522, w_module: 4.66717580052534\n",
      "iter: 510, loss: 0.10327500774121728, w_module: 4.707491718237643\n",
      "iter: 520, loss: 0.10145078552638004, w_module: 4.74715803877976\n",
      "iter: 530, loss: 0.09968988737320025, w_module: 4.78619389483896\n",
      "iter: 540, loss: 0.09798914208168316, w_module: 4.824617676107175\n",
      "iter: 550, loss: 0.09634558189349454, w_module: 4.862447060048657\n",
      "iter: 560, loss: 0.09475642680536119, w_module: 4.899699041891622\n",
      "iter: 570, loss: 0.093219070278911, w_module: 4.936389963737138\n",
      "iter: 580, loss: 0.09173106620741528, w_module: 4.972535542711246\n",
      "iter: 590, loss: 0.090290117015216, w_module: 5.008150898112314\n",
      "iter: 600, loss: 0.08889406277912253, w_module: 5.043250577526276\n",
      "iter: 610, loss: 0.08754087127298131, w_module: 5.0778485818986185\n",
      "iter: 620, loss: 0.08622862884714927, w_module: 5.111958389564642\n",
      "iter: 630, loss: 0.08495553206391399, w_module: 5.145592979249359\n",
      "iter: 640, loss: 0.08371988001814948, w_module: 5.1787648520559095\n",
      "iter: 650, loss: 0.0825200672798088, w_module: 5.211486052466941\n",
      "iter: 660, loss: 0.08135457740134236, w_module: 5.243768188387556\n",
      "iter: 670, loss: 0.08022197693889695, w_module: 5.275622450261375\n",
      "iter: 680, loss: 0.07912090994128188, w_module: 5.307059629293139\n",
      "iter: 690, loss: 0.07805009286525623, w_module: 5.338090134812519\n",
      "iter: 700, loss: 0.07700830987976581, w_module: 5.368724010814337\n",
      "iter: 710, loss: 0.07599440852539532, w_module: 5.398970951710508\n",
      "iter: 720, loss: 0.07500729569855129, w_module: 5.428840317328728\n",
      "iter: 730, loss: 0.07404593393279954, w_module: 5.458341147192355\n",
      "iter: 740, loss: 0.07310933795238438, w_module: 5.487482174115176\n",
      "iter: 750, loss: 0.07219657147529357, w_module: 5.5162718371437505\n",
      "iter: 760, loss: 0.07130674424532707, w_module: 5.544718293879033\n",
      "iter: 770, loss: 0.07043900927451287, w_module: 5.572829432207747\n",
      "iter: 780, loss: 0.06959256027890548, w_module: 5.600612881472889\n",
      "iter: 790, loss: 0.0687666292923296, w_module: 5.628076023111421\n",
      "iter: 800, loss: 0.06796048444400458, w_module: 5.655226000786077\n",
      "iter: 810, loss: 0.06717342788722715, w_module: 5.682069730036911\n",
      "iter: 820, loss: 0.06640479386740848, w_module: 5.708613907477059\n",
      "iter: 830, loss: 0.06565394691877607, w_module: 5.734865019555978\n",
      "iter: 840, loss: 0.06492028017996578, w_module: 5.760829350912296\n",
      "iter: 850, loss: 0.0642032138195593, w_module: 5.786512992337291\n",
      "iter: 860, loss: 0.06350219356337615, w_module: 5.811921848368939\n",
      "iter: 870, loss: 0.06281668931600995, w_module: 5.837061644535445\n",
      "iter: 880, loss: 0.06214619386972086, w_module: 5.861937934266181\n",
      "iter: 890, loss: 0.06149022169435736, w_module: 5.886556105487031\n",
      "iter: 900, loss: 0.060848307802495664, w_module: 5.910921386916215\n",
      "iter: 910, loss: 0.060220006684451086, w_module: 5.935038854075815\n",
      "iter: 920, loss: 0.05960489130824031, w_module: 5.958913435033435\n",
      "iter: 930, loss: 0.05900255217996429, w_module: 5.982549915887605\n",
      "iter: 940, loss: 0.05841259646043255, w_module: 6.0059529460098835\n",
      "iter: 950, loss: 0.05783464713417729, w_module: 6.029127043055798\n",
      "iter: 960, loss: 0.0572683422272981, w_module: 6.052076597756256\n",
      "iter: 970, loss: 0.05671333407085248, w_module: 6.074805878500292\n",
      "iter: 980, loss: 0.056169288606752674, w_module: 6.0973190357195275\n",
      "iter: 990, loss: 0.05563588473335918, w_module: 6.1196201060841\n"
     ]
    }
   ],
   "source": [
    "from logistic import LogisticRegression\n",
    "\n",
    "# create a LogisticRegression object \n",
    "LR = LogisticRegression()\n",
    "\n",
    "# fit the model to the training data without regularization (reg = 0)\n",
    "LR.fit(x_train, y_train, lr=0.1, n_iter=1000,reg=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![figure2](img/Figure_2.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行上述代码，你会发现，在不考虑正则化的情况下，$||w||$ 随着训练次数的增加会不断增大。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练完成后，你可以利用训练得到的分类器来进行预测。请你编写代码，计算训练集和测试集中的预测准确率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 1.0\n",
      "Test accuracy: 0.98\n"
     ]
    }
   ],
   "source": [
    "# Implement the code to compute the accuracy of logistic regression (LR) in the test set. Note that LR itself is already trained, if you have run the above code.\n",
    "\n",
    "# training accuracy\n",
    "\n",
    "# TODO: compute the y_pred using LR.predict() function\n",
    "\n",
    "x_train_bias = np.concatenate((x_train, np.ones((x_train.shape[0], 1))), axis=1)\n",
    "\n",
    "_, y_train_pred = LR.predict(x_train_bias)\n",
    "\n",
    "# TODO: compute the accuracy\n",
    "\n",
    "correct_train = np.sum(y_train == y_train_pred)\n",
    "train_acc = correct_train / len(y_train)\n",
    "\n",
    "\n",
    "print(\"Train accuracy: {}\".format(train_acc))\n",
    "\n",
    "\n",
    "# TODO: test accuracy, proceed similarly as above\n",
    "\n",
    "x_test_bias = np.concatenate((x_test, np.ones((x_test.shape[0], 1))), axis=1)\n",
    "\n",
    "_, y_test_pred = LR.predict(x_test_bias)\n",
    "\n",
    "correct_test = np.sum(y_test == y_test_pred)\n",
    "test_acc = correct_test / len(y_test)\n",
    "\n",
    "\n",
    "print(\"Test accuracy: {}\".format(test_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0, loss: 0.7622725867987941, w_module: 1.1667079684438386\n",
      "iter: 10, loss: 0.5509785407337849, w_module: 1.0064628543367549\n",
      "iter: 20, loss: 0.50295483351031, w_module: 1.044434020499834\n",
      "iter: 30, loss: 0.47248385816908595, w_module: 1.1313783808678242\n",
      "iter: 40, loss: 0.44505956252185563, w_module: 1.2340809214330062\n",
      "iter: 50, loss: 0.42005578290569234, w_module: 1.3440599578686914\n",
      "iter: 60, loss: 0.39723016022944657, w_module: 1.4573145786875048\n",
      "iter: 70, loss: 0.3763674267542891, w_module: 1.5714264845191692\n",
      "iter: 80, loss: 0.3572718709250485, w_module: 1.6848608816901165\n",
      "iter: 90, loss: 0.339766544266536, w_module: 1.7966445852589388\n",
      "iter: 100, loss: 0.3236922426584391, w_module: 1.9061717856943128\n",
      "iter: 110, loss: 0.3089062474059734, w_module: 2.013079423707028\n",
      "iter: 120, loss: 0.29528094826229295, w_module: 2.117165889039024\n",
      "iter: 130, loss: 0.2827024398219354, w_module: 2.2183373896771297\n",
      "iter: 140, loss: 0.2710691525160726, w_module: 2.316572176308907\n",
      "iter: 150, loss: 0.26029055636737014, w_module: 2.4118964145100654\n",
      "iter: 160, loss: 0.25028595898475786, w_module: 2.504367750789083\n",
      "iter: 170, loss: 0.24098340766835508, w_module: 2.594064027558047\n",
      "iter: 180, loss: 0.2323186977122231, w_module: 2.681075487895407\n",
      "iter: 190, loss: 0.22423448400891235, w_module: 2.7654993735041273\n",
      "iter: 200, loss: 0.21667949006699497, w_module: 2.8474361810706053\n",
      "iter: 210, loss: 0.2096078069388956, w_module: 2.9269870781599865\n",
      "iter: 220, loss: 0.20297827387041445, w_module: 3.0042521358029766\n",
      "iter: 230, loss: 0.196753932400809, w_module: 3.0793291395219193\n",
      "iter: 240, loss: 0.19090154593446163, w_module: 3.1523128115759182\n",
      "iter: 250, loss: 0.18539117731421914, w_module: 3.2232943260319775\n",
      "iter: 260, loss: 0.18019581754602995, w_module: 3.292361032209104\n",
      "iter: 270, loss: 0.17529105948466184, w_module: 3.359596325874957\n",
      "iter: 280, loss: 0.1706548109467834, w_module: 3.4250796244640944\n",
      "iter: 290, loss: 0.16626704234384196, w_module: 3.48888641465436\n",
      "iter: 300, loss: 0.16210956450818859, w_module: 3.551088349322812\n",
      "iter: 310, loss: 0.15816583291484296, w_module: 3.611753377192452\n",
      "iter: 320, loss: 0.15442077497627346, w_module: 3.670945893060973\n",
      "iter: 330, loss: 0.15086063750983586, w_module: 3.7287268998524357\n",
      "iter: 340, loss: 0.14747285185012113, w_module: 3.7851541761913614\n",
      "iter: 350, loss: 0.1442459144054259, w_module: 3.8402824450077895\n",
      "iter: 360, loss: 0.14116928074328156, w_module: 3.894163540014975\n",
      "iter: 370, loss: 0.1382332715389001, w_module: 3.9468465678841183\n",
      "iter: 380, loss: 0.13542898893677663, w_module: 3.9983780646642457\n",
      "iter: 390, loss: 0.13274824206350108, w_module: 4.048802145526919\n",
      "iter: 400, loss: 0.13018348059267912, w_module: 4.098160647304266\n",
      "iter: 410, loss: 0.12772773540399088, w_module: 4.146493263571127\n",
      "iter: 420, loss: 0.12537456550068934, w_module: 4.193837672225162\n",
      "iter: 430, loss: 0.12311801045579611, w_module: 4.240229655662531\n",
      "iter: 440, loss: 0.1209525477490862, w_module: 4.285703213746547\n",
      "iter: 450, loss: 0.11887305443659679, w_module: 4.330290669833586\n",
      "iter: 460, loss: 0.116874772663508, w_module: 4.3740227701632\n",
      "iter: 470, loss: 0.11495327859126624, w_module: 4.416928776944126\n",
      "iter: 480, loss: 0.11310445436200037, w_module: 4.459036555479488\n",
      "iter: 490, loss: 0.11132446276867869, w_module: 4.5003726556767045\n",
      "iter: 500, loss: 0.10960972433900526, w_module: 4.540962388282981\n",
      "iter: 510, loss: 0.10795689657553957, w_module: 4.580829896178019\n",
      "iter: 520, loss: 0.10636285512463255, w_module: 4.619998221043111\n",
      "iter: 530, loss: 0.10482467667309252, w_module: 4.658489365711279\n",
      "iter: 540, loss: 0.10333962339452626, w_module: 4.69632435248752\n",
      "iter: 550, loss: 0.10190512878748724, w_module: 4.733523277711906\n",
      "iter: 560, loss: 0.10051878476527215, w_module: 4.770105362822055\n",
      "iter: 570, loss: 0.09917832987276415, w_module: 4.806089002155327\n",
      "iter: 580, loss: 0.0978816385194096, w_module: 4.841491807715454\n",
      "iter: 590, loss: 0.09662671112946837, w_module: 4.876330651113269\n",
      "iter: 600, loss: 0.0954116651213089, w_module: 4.910621702876792\n",
      "iter: 610, loss: 0.09423472663690781, w_module: 4.944380469312377\n",
      "iter: 620, loss: 0.0930942229510133, w_module: 4.977621827085764\n",
      "iter: 630, loss: 0.091988575496781, w_module: 5.010360055679831\n",
      "iter: 640, loss: 0.09091629345120425, w_module: 5.042608867874604\n",
      "iter: 650, loss: 0.08987596782944206, w_module: 5.074381438384557\n",
      "iter: 660, loss: 0.08886626604228505, w_module: 5.105690430778444\n",
      "iter: 670, loss: 0.08788592687557006, w_module: 5.136548022797835\n",
      "iter: 680, loss: 0.08693375585442588, w_module: 5.166965930182053\n",
      "iter: 690, loss: 0.0860086209588639, w_module: 5.19695542909943\n",
      "iter: 700, loss: 0.08510944866046881, w_module: 5.2265273772775\n",
      "iter: 710, loss: 0.08423522025284313, w_module: 5.255692233918106\n",
      "iter: 720, loss: 0.08338496845105409, w_module: 5.284460078477122\n",
      "iter: 730, loss: 0.08255777423765384, w_module: 5.312840628382812\n",
      "iter: 740, loss: 0.08175276393493013, w_module: 5.340843255761519\n",
      "iter: 750, loss: 0.08096910648491572, w_module: 5.368477003234469\n",
      "iter: 760, loss: 0.08020601092036787, w_module: 5.395750598844963\n",
      "iter: 770, loss: 0.07946272401144362, w_module: 5.422672470171011\n",
      "iter: 780, loss: 0.07873852807416101, w_module: 5.449250757674616\n",
      "iter: 790, loss: 0.07803273892796556, w_module: 5.47549332733534\n",
      "iter: 800, loss: 0.07734470399083392, w_module: 5.501407782612452\n",
      "iter: 810, loss: 0.07667380050134799, w_module: 5.527001475776868\n",
      "iter: 820, loss: 0.07601943385808176, w_module: 5.552281518651385\n",
      "iter: 830, loss: 0.07538103606746475, w_module: 5.577254792794895\n",
      "iter: 840, loss: 0.07475806429203026, w_module: 5.601927959164032\n",
      "iter: 850, loss: 0.07414999949163278, w_module: 5.626307467283311\n",
      "iter: 860, loss: 0.0735563451508323, w_module: 5.65039956395283\n",
      "iter: 870, loss: 0.07297662608619997, w_module: 5.67421030152062\n",
      "iter: 880, loss: 0.0724103873278081, w_module: 5.697745545744957\n",
      "iter: 890, loss: 0.0718571930696272, w_module: 5.721010983270318\n",
      "iter: 900, loss: 0.07131662568397572, w_module: 5.744012128739061\n",
      "iter: 910, loss: 0.07078828479554905, w_module: 5.766754331559546\n",
      "iter: 920, loss: 0.07027178641090796, w_module: 5.789242782350028\n",
      "iter: 930, loss: 0.06976676209962249, w_module: 5.8114825190764785\n",
      "iter: 940, loss: 0.06927285822356351, w_module: 5.833478432901271\n",
      "iter: 950, loss: 0.06878973521109932, w_module: 5.855235273758684\n",
      "iter: 960, loss: 0.06831706687320041, w_module: 5.876757655672118\n",
      "iter: 970, loss: 0.06785453975868086, w_module: 5.898050061826996\n",
      "iter: 980, loss: 0.06740185254600918, w_module: 5.919116849412549\n",
      "iter: 990, loss: 0.06695871546931208, w_module: 5.939962254244733\n"
     ]
    }
   ],
   "source": [
    "# create a LogisticRegression object and train it when using regularization\n",
    "LR = LogisticRegression()\n",
    "LR.fit(x_train, y_train, lr=0.1, n_iter=1000,reg=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![figure3](img/Figure_3.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 1.0\n",
      "Test accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement the code to compute the accuracy of logistic regression (LR) in the test set. Note that LR itself is already trained, if you have run the above code.\n",
    "\n",
    "x_train_bias = np.concatenate((x_train, np.ones((x_train.shape[0], 1))), axis=1)\n",
    "\n",
    "_, y_train_pred = LR.predict(x_train_bias)\n",
    "\n",
    "# TODO: compute the accuracy\n",
    "\n",
    "correct_train = np.sum(y_train == y_train_pred)\n",
    "train_acc = correct_train / len(y_train)\n",
    "\n",
    "\n",
    "print(\"Train accuracy: {}\".format(train_acc))\n",
    "\n",
    "\n",
    "# TODO: test accuracy, proceed similarly as above\n",
    "\n",
    "x_test_bias = np.concatenate((x_test, np.ones((x_test.shape[0], 1))), axis=1)\n",
    "\n",
    "_, y_test_pred = LR.predict(x_test_bias)\n",
    "\n",
    "correct_test = np.sum(y_test == y_test_pred)\n",
    "test_acc = correct_test / len(y_test)\n",
    "\n",
    "\n",
    "print(\"Test accuracy: {}\".format(test_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行上述带有正则化的代码后，请观察 $||w||$ 的变化，并讨论正则化的实际意义。(请将答案写在下方)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "w的增加变慢了一些，使得模型的鲁棒性更强一些，最大程度限制过拟合"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
