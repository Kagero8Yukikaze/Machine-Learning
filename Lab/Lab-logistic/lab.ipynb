{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备工作\n",
    "### 环境准备\n",
    "\n",
    "请确保完成以下依赖包的安装，并且通过下面代码来导入与验证。运行成功后，你会看到一个新的窗口，其展示了一张空白的figure。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, List\n",
    "\n",
    "# display the plot in a separate window\n",
    "%matplotlib tk\n",
    "\n",
    "np.random.seed(12)\n",
    "\n",
    "# create a figure and axis\n",
    "plt.ion()\n",
    "fig = plt.figure(figsize=(12, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据集准备\n",
    "\n",
    "你将使用以下二维数据集来训练逻辑分类器，并观察随着训练的进行，线性分割面的变化。\n",
    "\n",
    "该数据集包含两个特征和一个标签，其中标签 $ y \\in \\{-1,1\\} $。\n",
    "\n",
    "请执行下面的代码以加载数据集并对其进行可视化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_generator import gen_2D_dataset\n",
    "\n",
    "x_train, y_train = gen_2D_dataset(100, 100, noise = 0)\n",
    "x_test, y_test = gen_2D_dataset(50, 50, noise = 0.7) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vis_util import visualize_2D_dataset, visualize_2D_border\n",
    "\n",
    "visualize_2D_dataset(x_train, y_train)\n",
    "visualize_2D_dataset(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![figure1](img/Figure_1.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 逻辑回归 (10 pts)\n",
    "\n",
    "在这一部分，你将学习并完成逻辑回归相关代码的编写与训练。\n",
    "\n",
    "在运行这部分代码之前，请确保你已经完成了 `logistics.py` 文件的代码补全。\n",
    "\n",
    "完成后，运行以下代码，你会看到一张figure来展示$||w||$，loss和决策边界的变化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0, loss: 0.6807266354549091, w_module: 1.86503565781866\n",
      "iter: 10, loss: 0.6047988783795631, w_module: 1.8481489888845297\n",
      "iter: 20, loss: 0.5660832091387538, w_module: 1.8416649857462604\n",
      "iter: 30, loss: 0.5309314279730247, w_module: 1.852628762958914\n",
      "iter: 40, loss: 0.49873990315881916, w_module: 1.8798975391623565\n",
      "iter: 50, loss: 0.4692664495478231, w_module: 1.9209134420153\n",
      "iter: 60, loss: 0.4422816596337917, w_module: 1.9730364389416215\n",
      "iter: 70, loss: 0.4175668558824793, w_module: 2.033845587213826\n",
      "iter: 80, loss: 0.39491642826729456, w_module: 2.101235926921896\n",
      "iter: 90, loss: 0.3741392267477511, w_module: 2.173440566023107\n",
      "iter: 100, loss: 0.3550591551622385, w_module: 2.249013982747482\n",
      "iter: 110, loss: 0.33751516416978994, w_module: 2.326796185724719\n",
      "iter: 120, loss: 0.32136081353797313, w_module: 2.4058701532487836\n",
      "iter: 130, loss: 0.30646354486334976, w_module: 2.4855197670708282\n",
      "iter: 140, loss: 0.29270377665622754, w_module: 2.5651918359489785\n",
      "iter: 150, loss: 0.2799739069601067, w_module: 2.6444635594027073\n",
      "iter: 160, loss: 0.26817728569910904, w_module: 2.723015541903783\n",
      "iter: 170, loss: 0.2572272001987447, w_module: 2.80060987039218\n",
      "iter: 180, loss: 0.24704590264325801, w_module: 2.877072542545002\n",
      "iter: 190, loss: 0.23756369713488554, w_module: 2.952279503475223\n",
      "iter: 200, loss: 0.2287180959040338, w_module: 3.026145610160481\n",
      "iter: 210, loss: 0.22045304847935793, w_module: 3.098615938595309\n",
      "iter: 220, loss: 0.21271824371709563, w_module: 3.169658949317714\n",
      "iter: 230, loss: 0.20546848204891383, w_module: 3.239261119331016\n",
      "iter: 240, loss: 0.19866311376522533, w_module: 3.3074227276783668\n",
      "iter: 250, loss: 0.19226553831674806, w_module: 3.3741545473463113\n",
      "iter: 260, loss: 0.1862427592713795, w_module: 3.439475248947205\n",
      "iter: 270, loss: 0.1805649895436243, w_module: 3.503409363592368\n",
      "iter: 280, loss: 0.17520530170196288, w_module: 3.565985685435037\n",
      "iter: 290, loss: 0.17013931847203295, w_module: 3.6272360202793323\n",
      "iter: 300, loss: 0.16534493893221425, w_module: 3.6871942069037673\n",
      "iter: 310, loss: 0.16080209630385894, w_module: 3.745895353552433\n",
      "iter: 320, loss: 0.15649254364519968, w_module: 3.8033752443787776\n",
      "iter: 330, loss: 0.1523996641497105, w_module: 3.8596698802551885\n",
      "iter: 340, loss: 0.1485083031168994, w_module: 3.914815125888402\n",
      "iter: 350, loss: 0.14480461900122626, w_module: 3.968846441074419\n",
      "iter: 360, loss: 0.14127595125119785, w_module: 4.0217986785501445\n",
      "iter: 370, loss: 0.13791070292575736, w_module: 4.073705934533657\n",
      "iter: 380, loss: 0.13469823632016772, w_module: 4.12460144090855\n",
      "iter: 390, loss: 0.13162878005067505, w_module: 4.17451749026865\n",
      "iter: 400, loss: 0.1286933462386804, w_module: 4.223485386828482\n",
      "iter: 410, loss: 0.12588365660340467, w_module: 4.2715354176234905\n",
      "iter: 420, loss: 0.12319207641954594, w_module: 4.318696839551391\n",
      "iter: 430, loss: 0.12061155542551867, w_module: 4.36499787870358\n",
      "iter: 440, loss: 0.11813557488069022, w_module: 4.410465739151573\n",
      "iter: 450, loss: 0.11575810006855622, w_module: 4.455126618925515\n",
      "iter: 460, loss: 0.11347353762879621, w_module: 4.499005731379726\n",
      "iter: 470, loss: 0.11127669717620334, w_module: 4.542127330507203\n",
      "iter: 480, loss: 0.10916275672998427, w_module: 4.584514739059417\n",
      "iter: 490, loss: 0.10712723153411047, w_module: 4.6261903785643\n",
      "iter: 500, loss: 0.10516594589934522, w_module: 4.667175800525339\n",
      "iter: 510, loss: 0.10327500774121728, w_module: 4.707491718237644\n",
      "iter: 520, loss: 0.10145078552638004, w_module: 4.7471580387797605\n",
      "iter: 530, loss: 0.09968988737320025, w_module: 4.78619389483896\n",
      "iter: 540, loss: 0.09798914208168316, w_module: 4.824617676107175\n",
      "iter: 550, loss: 0.09634558189349454, w_module: 4.862447060048657\n",
      "iter: 560, loss: 0.09475642680536119, w_module: 4.899699041891622\n",
      "iter: 570, loss: 0.093219070278911, w_module: 4.936389963737138\n",
      "iter: 580, loss: 0.09173106620741528, w_module: 4.972535542711247\n",
      "iter: 590, loss: 0.090290117015216, w_module: 5.008150898112314\n",
      "iter: 600, loss: 0.08889406277912253, w_module: 5.043250577526275\n",
      "iter: 610, loss: 0.08754087127298131, w_module: 5.0778485818986185\n",
      "iter: 620, loss: 0.08622862884714927, w_module: 5.111958389564642\n",
      "iter: 630, loss: 0.08495553206391399, w_module: 5.145592979249359\n",
      "iter: 640, loss: 0.08371988001814948, w_module: 5.1787648520559095\n",
      "iter: 650, loss: 0.0825200672798088, w_module: 5.211486052466941\n",
      "iter: 660, loss: 0.08135457740134236, w_module: 5.243768188387556\n",
      "iter: 670, loss: 0.08022197693889695, w_module: 5.275622450261374\n",
      "iter: 680, loss: 0.07912090994128188, w_module: 5.307059629293139\n",
      "iter: 690, loss: 0.07805009286525623, w_module: 5.338090134812519\n",
      "iter: 700, loss: 0.07700830987976581, w_module: 5.368724010814337\n",
      "iter: 710, loss: 0.07599440852539532, w_module: 5.398970951710508\n",
      "iter: 720, loss: 0.07500729569855129, w_module: 5.428840317328728\n",
      "iter: 730, loss: 0.07404593393279954, w_module: 5.458341147192354\n",
      "iter: 740, loss: 0.07310933795238438, w_module: 5.487482174115176\n",
      "iter: 750, loss: 0.07219657147529357, w_module: 5.5162718371437505\n",
      "iter: 760, loss: 0.07130674424532707, w_module: 5.544718293879033\n",
      "iter: 770, loss: 0.07043900927451287, w_module: 5.572829432207747\n",
      "iter: 780, loss: 0.06959256027890548, w_module: 5.600612881472889\n",
      "iter: 790, loss: 0.0687666292923296, w_module: 5.628076023111421\n",
      "iter: 800, loss: 0.06796048444400458, w_module: 5.655226000786078\n",
      "iter: 810, loss: 0.06717342788722715, w_module: 5.682069730036911\n",
      "iter: 820, loss: 0.06640479386740848, w_module: 5.708613907477059\n",
      "iter: 830, loss: 0.06565394691877607, w_module: 5.734865019555978\n",
      "iter: 840, loss: 0.06492028017996578, w_module: 5.760829350912296\n",
      "iter: 850, loss: 0.0642032138195593, w_module: 5.786512992337291\n",
      "iter: 860, loss: 0.06350219356337615, w_module: 5.811921848368939\n",
      "iter: 870, loss: 0.06281668931600995, w_module: 5.837061644535445\n",
      "iter: 880, loss: 0.06214619386972086, w_module: 5.861937934266181\n",
      "iter: 890, loss: 0.06149022169435736, w_module: 5.886556105487031\n",
      "iter: 900, loss: 0.060848307802495664, w_module: 5.910921386916215\n",
      "iter: 910, loss: 0.060220006684451086, w_module: 5.935038854075815\n",
      "iter: 920, loss: 0.05960489130824031, w_module: 5.958913435033435\n",
      "iter: 930, loss: 0.05900255217996429, w_module: 5.982549915887605\n",
      "iter: 940, loss: 0.05841259646043255, w_module: 6.0059529460098835\n",
      "iter: 950, loss: 0.05783464713417729, w_module: 6.029127043055797\n",
      "iter: 960, loss: 0.0572683422272981, w_module: 6.052076597756256\n",
      "iter: 970, loss: 0.05671333407085248, w_module: 6.074805878500292\n",
      "iter: 980, loss: 0.056169288606752674, w_module: 6.0973190357195275\n",
      "iter: 990, loss: 0.05563588473335918, w_module: 6.1196201060841\n"
     ]
    }
   ],
   "source": [
    "from logistic import LogisticRegression\n",
    "\n",
    "# create a LogisticRegression object \n",
    "LR = LogisticRegression()\n",
    "\n",
    "# fit the model to the training data without regularization (reg = 0)\n",
    "LR.fit(x_train, y_train, lr=0.1, n_iter=1000,reg=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![figure2](img/Figure_2.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行上述代码，你会发现，在不考虑正则化的情况下，$||w||$ 随着训练次数的增加会不断增大。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练完成后，你可以利用训练得到的分类器来进行预测。请你编写代码，计算训练集和测试集中的预测准确率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 1.0\n",
      "Test accuracy: 0.98\n"
     ]
    }
   ],
   "source": [
    "# Implement the code to compute the accuracy of logistic regression (LR) in the test set. Note that LR itself is already trained, if you have run the above code.\n",
    "\n",
    "# training accuracy\n",
    "\n",
    "# TODO: compute the y_pred using LR.predict() function\n",
    "\n",
    "x_train_bias = np.concatenate((x_train, np.ones((x_train.shape[0], 1))), axis=1)\n",
    "\n",
    "_, y_train_pred = LR.predict(x_train_bias)\n",
    "\n",
    "# TODO: compute the accuracy\n",
    "\n",
    "correct_train = np.sum(y_train == y_train_pred)\n",
    "train_acc = correct_train / len(y_train)\n",
    "\n",
    "\n",
    "print(\"Train accuracy: {}\".format(train_acc))\n",
    "\n",
    "\n",
    "# TODO: test accuracy, proceed similarly as above\n",
    "\n",
    "x_test_bias = np.concatenate((x_test, np.ones((x_test.shape[0], 1))), axis=1)\n",
    "\n",
    "_, y_test_pred = LR.predict(x_test_bias)\n",
    "\n",
    "correct_test = np.sum(y_test == y_test_pred)\n",
    "test_acc = correct_test / len(y_test)\n",
    "\n",
    "\n",
    "print(\"Test accuracy: {}\".format(test_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0, loss: 1.0359734045322084, w_module: 0.9099088363124627\n",
      "iter: 10, loss: 0.748761299151733, w_module: 0.7208561735308843\n",
      "iter: 20, loss: 0.6958635342262046, w_module: 0.530628627712227\n",
      "iter: 30, loss: 0.6482742878571777, w_module: 0.3756706864340863\n",
      "iter: 40, loss: 0.6048333779052433, w_module: 0.30766481420772807\n",
      "iter: 50, loss: 0.5652507493929028, w_module: 0.36197731850752757\n",
      "iter: 60, loss: 0.5292284278469921, w_module: 0.4891588337932561\n",
      "iter: 70, loss: 0.49646584517558784, w_module: 0.6398554720594865\n",
      "iter: 80, loss: 0.4666691974963134, w_module: 0.7953060040964518\n",
      "iter: 90, loss: 0.4395581224185239, w_module: 0.9490342729607233\n",
      "iter: 100, loss: 0.4148700146911712, w_module: 1.0986122236605878\n",
      "iter: 110, loss: 0.3923624203183465, w_module: 1.2431099063019668\n",
      "iter: 120, loss: 0.3718139643525305, w_module: 1.3822276226309793\n",
      "iter: 130, loss: 0.3530242275023076, w_module: 1.5159566408570258\n",
      "iter: 140, loss: 0.33581291837085325, w_module: 1.644429799208971\n",
      "iter: 150, loss: 0.3200186127410241, w_module: 1.767849292722379\n",
      "iter: 160, loss: 0.3054972612249636, w_module: 1.8864492466322162\n",
      "iter: 170, loss: 0.29212060747468344, w_module: 2.0004753535108937\n",
      "iter: 180, loss: 0.27977461245270624, w_module: 2.110173474687921\n",
      "iter: 190, loss: 0.2683579451573455, w_module: 2.2157832145425833\n",
      "iter: 200, loss: 0.2577805748687264, w_module: 2.3175343712105962\n",
      "iter: 210, loss: 0.2479624823819704, w_module: 2.415645101974767\n",
      "iter: 220, loss: 0.23883249592868278, w_module: 2.510321130386713\n",
      "iter: 230, loss: 0.23032724997032733, w_module: 2.601755591043656\n",
      "iter: 240, loss: 0.2223902605278696, w_module: 2.690129262466726\n",
      "iter: 250, loss: 0.21497110824873297, w_module: 2.7756110307020307\n",
      "iter: 260, loss: 0.20802471931637273, w_module: 2.858358482992428\n",
      "iter: 270, loss: 0.20151073409012685, w_module: 2.9385185666835105\n",
      "iter: 280, loss: 0.19539295368340542, w_module: 3.0162282716056867\n",
      "iter: 290, loss: 0.18963885531580052, w_module: 3.0916153092780227\n",
      "iter: 300, loss: 0.1842191680562587, w_module: 3.164798772273821\n",
      "iter: 310, loss: 0.17910750141145604, w_module: 3.2358897637447686\n",
      "iter: 320, loss: 0.17428002004475915, w_module: 3.3049919915435155\n",
      "iter: 330, loss: 0.1697151587007206, w_module: 3.3722023243410653\n",
      "iter: 340, loss: 0.16539337213869726, w_module: 3.437611309084708\n",
      "iter: 350, loss: 0.16129691553847572, w_module: 3.501303650405144\n",
      "iter: 360, loss: 0.15740965142903915, w_module: 3.563358653375109\n",
      "iter: 370, loss: 0.15371687971120365, w_module: 3.623850631494808\n",
      "iter: 380, loss: 0.15020518780040376, w_module: 3.6828492820341086\n",
      "iter: 390, loss: 0.14686231831315277, w_module: 3.740420030968756\n",
      "iter: 400, loss: 0.14367705206572862, w_module: 3.796624349757107\n",
      "iter: 410, loss: 0.14063910445245686, w_module: 3.8515200461493744\n",
      "iter: 420, loss: 0.1377390335292541, w_module: 3.9051615311270194\n",
      "iter: 430, loss: 0.13496815835105802, w_module: 3.9576000639521727\n",
      "iter: 440, loss: 0.13231848630410115, w_module: 4.008883977177284\n",
      "iter: 450, loss: 0.1297826483398249, w_module: 4.059058883331138\n",
      "iter: 460, loss: 0.12735384116024112, w_module: 4.108167864864224\n",
      "iter: 470, loss: 0.1250257755279, w_module: 4.156251648807456\n",
      "iter: 480, loss: 0.12279262998007975, w_module: 4.203348767475528\n",
      "iter: 490, loss: 0.12064900931875358, w_module: 4.249495706430807\n",
      "iter: 500, loss: 0.11858990732735575, w_module: 4.2947270408164195\n",
      "iter: 510, loss: 0.11661067323412802, w_module: 4.339075561067948\n",
      "iter: 520, loss: 0.11470698150138223, w_module: 4.382572388922017\n",
      "iter: 530, loss: 0.11287480457165533, w_module: 4.42524708455661\n",
      "iter: 540, loss: 0.11111038824657471, w_module: 4.467127745621782\n",
      "iter: 550, loss: 0.10941022941322207, w_module: 4.508241098850198\n",
      "iter: 560, loss: 0.10777105586671981, w_module: 4.548612584873873\n",
      "iter: 570, loss: 0.10618980800732887, w_module: 4.588266436816459\n",
      "iter: 580, loss: 0.10466362221616245, w_module: 4.627225753178567\n",
      "iter: 590, loss: 0.1031898157361765, w_module: 4.665512565486748\n",
      "iter: 600, loss: 0.10176587290484079, w_module: 4.703147901134323\n",
      "iter: 610, loss: 0.10038943260219974, w_module: 4.740151841803775\n",
      "iter: 620, loss: 0.09905827679321587, w_module: 4.776543577825661\n",
      "iter: 630, loss: 0.09777032005663777, w_module: 4.812341458797552\n",
      "iter: 640, loss: 0.09652360000437846, w_module: 4.847563040757946\n",
      "iter: 650, loss: 0.09531626850574174, w_module: 4.882225130184378\n",
      "iter: 660, loss: 0.0941465836399687, w_module: 4.91634382506148\n",
      "iter: 670, loss: 0.09301290230864802, w_module: 4.949934553243681\n",
      "iter: 680, loss: 0.0919136734466765, w_module: 4.983012108317907\n",
      "iter: 690, loss: 0.09084743177678296, w_module: 5.015590683154296\n",
      "iter: 700, loss: 0.08981279205824369, w_module: 5.0476839013171295\n",
      "iter: 710, loss: 0.08880844378540399, w_module: 5.079304846493746\n",
      "iter: 720, loss: 0.08783314629605538, w_module: 5.110466090086227\n",
      "iter: 730, loss: 0.08688572425366763, w_module: 5.141179717098745\n",
      "iter: 740, loss: 0.08596506347099803, w_module: 5.171457350442607\n",
      "iter: 750, loss: 0.08507010704574211, w_module: 5.201310173771283\n",
      "iter: 760, loss: 0.08419985178170197, w_module: 5.230748952948649\n",
      "iter: 770, loss: 0.08335334487146158, w_module: 5.25978405624554\n",
      "iter: 780, loss: 0.08252968081881296, w_module: 5.288425473352215\n",
      "iter: 790, loss: 0.0817279985811946, w_module: 5.316682833287592\n",
      "iter: 800, loss: 0.08094747891422178, w_module: 5.34456542127972\n",
      "iter: 810, loss: 0.08018734190201589, w_module: 5.372082194686447\n",
      "iter: 820, loss: 0.07944684465850911, w_module: 5.3992417980198715\n",
      "iter: 830, loss: 0.07872527918622317, w_module: 5.426052577133454\n",
      "iter: 840, loss: 0.07802197038021204, w_module: 5.452522592626283\n",
      "iter: 850, loss: 0.07733627416593598, w_module: 5.478659632514933\n",
      "iter: 860, loss: 0.07666757576080652, w_module: 5.504471224219683\n",
      "iter: 870, loss: 0.07601528805002111, w_module: 5.5299646459085015\n",
      "iter: 880, loss: 0.07537885006810456, w_module: 5.555146937239\n",
      "iter: 890, loss: 0.07475772557829494, w_module: 5.580024909535801\n",
      "iter: 900, loss: 0.07415140174256685, w_module: 5.604605155438027\n",
      "iter: 910, loss: 0.07355938787568092, w_module: 5.62889405804924\n",
      "iter: 920, loss: 0.07298121427718779, w_module: 5.6528977996198995\n",
      "iter: 930, loss: 0.0724164311358074, w_module: 5.67662236979033\n",
      "iter: 940, loss: 0.07186460750105181, w_module: 5.700073573420306\n",
      "iter: 950, loss: 0.07132533031736854, w_module: 5.723257038029547\n",
      "iter: 960, loss: 0.07079820351645286, w_module: 5.746178220871849\n",
      "iter: 970, loss: 0.0702828471637182, w_module: 5.768842415664002\n",
      "iter: 980, loss: 0.06977889665522359, w_module: 5.791254758989289\n",
      "iter: 990, loss: 0.06928600196164167, w_module: 5.813420236394061\n"
     ]
    }
   ],
   "source": [
    "# create a LogisticRegression object and train it when using regularization\n",
    "LR = LogisticRegression()\n",
    "LR.fit(x_train, y_train, lr=0.1, n_iter=1000,reg=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![figure3](img/Figure_3.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 1.0\n",
      "Test accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement the code to compute the accuracy of logistic regression (LR) in the test set. Note that LR itself is already trained, if you have run the above code.\n",
    "\n",
    "x_train_bias = np.concatenate((x_train, np.ones((x_train.shape[0], 1))), axis=1)\n",
    "\n",
    "_, y_train_pred = LR.predict(x_train_bias)\n",
    "\n",
    "# TODO: compute the accuracy\n",
    "\n",
    "correct_train = np.sum(y_train == y_train_pred)\n",
    "train_acc = correct_train / len(y_train)\n",
    "\n",
    "\n",
    "print(\"Train accuracy: {}\".format(train_acc))\n",
    "\n",
    "\n",
    "# TODO: test accuracy, proceed similarly as above\n",
    "\n",
    "x_test_bias = np.concatenate((x_test, np.ones((x_test.shape[0], 1))), axis=1)\n",
    "\n",
    "_, y_test_pred = LR.predict(x_test_bias)\n",
    "\n",
    "correct_test = np.sum(y_test == y_test_pred)\n",
    "test_acc = correct_test / len(y_test)\n",
    "\n",
    "\n",
    "print(\"Test accuracy: {}\".format(test_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行上述带有正则化的代码后，请观察 $||w||$ 的变化，并讨论正则化的实际意义。(请将答案写在下方)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "w的增加变慢了一些，通过限制w的增长，使得模型的鲁棒性更强一些，最大程度限制过拟合  \n",
    "若提高lr/reg的比值，w的收敛速度会更快一些，比如当lr=1, reg=0.1时，收敛效果比较明显"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
